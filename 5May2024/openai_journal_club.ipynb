{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journal Club May 3rd 2024\n",
    "This is an example notebook where I test how important correct labels are for few shot learning with GPT3.5. This was inspired by the journal club paper.\n",
    "\n",
    "In this first section, I think I am prompting the model in the \"right\" way. For this chat model, I provide the instructions in the system section at the top, then the examples as conversations back and forth between a user and the model. Then I end with the query text as a user prompt. Hopefully this properly demonstrates the format of the task. \n",
    "\n",
    "Below, under OLD, I tried a different format where the instruction and all examples were fed in as a single user prompt. I feel like this is a less good way to prompt the model, however it seemed to perform maybe slightly better. Overall, I'm not totally sure what the right way is to prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(examples, query):\n",
    "    messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Provide a label indicating the sentiment of the user's text. Respond with only the label.\"},\n",
    "        ]\n",
    "    for example in examples:\n",
    "        messages.append({\"role\": \"user\",   \"content\": example[0]})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": example[1]})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of correct vs. incorrect labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "query = \"I like dogs.\"\n",
    "\n",
    "examples = [\n",
    "    (\"I like cats.\",\"positive\"),\n",
    "    (\"I dislike birds.\",\"negative\"),\n",
    "]\n",
    "print(gpt(examples, query))\n",
    "\n",
    "examples = [\n",
    "    (\"I like cats.\",\"negative\"),\n",
    "    (\"I dislike birds.\",\"positive\"),\n",
    "]\n",
    "print(gpt(examples, query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A still pretty simple example of correct vs. incorrect labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "query = \"I think that birds are really cool.\"\n",
    "\n",
    "examples = [\n",
    "    (\"I want to get a cat someday.\",\"positive\"),\n",
    "    (\"Dogs are way too loud for me.\",\"negative\"),\n",
    "]\n",
    "print(gpt(examples, query))\n",
    "\n",
    "examples = [\n",
    "    (\"I want to get a cat someday.\",\"negative\"),\n",
    "    (\"Dogs are way too loud for me.\",\"positive\"),\n",
    "]\n",
    "print(gpt(examples, query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrating labels. If I don't specify the neutral label in any examples, can the model still label the query as neutral? How much does random versus correct labels influence this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'neutral', 'neutral', 'neutral', 'neutral']\n",
      "['neutral', 'positive', 'positive', 'positive', 'positive']\n",
      "['neutral', 'neutral', 'neutral', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "def run_five_times(examples, query):\n",
    "    lst = []\n",
    "    for _ in range(5):\n",
    "        lst.append(gpt(examples, query))\n",
    "    lst.sort()\n",
    "    return lst\n",
    "\n",
    "query = \"Although I don't want one as a pet, birds aren't the worst.\"\n",
    "\n",
    "examples = [\n",
    "    (\"I want to get a cat someday.\",\"positive\"),\n",
    "    (\"Dogs are way too loud for me.\",\"negative\"),\n",
    "    (\"Aren't peacocks really pretty?\",\"positive\"),\n",
    "    (\"I think that lions are terrifying.\",\"negative\"),\n",
    "]\n",
    "print(run_five_times(examples, query))\n",
    "\n",
    "examples = [\n",
    "    (\"I want to get a cat someday.\",\"positive\"),\n",
    "    (\"Dogs are way too loud for me.\",\"positive\"),\n",
    "    (\"Aren't peacocks really pretty?\",\"positive\"),\n",
    "    (\"I think that lions are terrifying.\",\"positive\"),\n",
    "]\n",
    "print(run_five_times(examples, query))\n",
    "\n",
    "examples = [\n",
    "    (\"I want to get a cat someday.\",\"neutral\"),\n",
    "    (\"Dogs are way too loud for me.\",\"positive\"),\n",
    "    (\"Aren't peacocks really pretty?\",\"neutral\"),\n",
    "    (\"I think that lions are terrifying.\",\"neutral\"),\n",
    "]\n",
    "print(run_five_times(examples, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD\n",
    "\n",
    "I think the above formatting is more correct than the below formatting, but I am not totally sure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": text},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I like cats.\\nLabel: positive\n",
    "Text: I dislike birds.\\nLabel: negative\n",
    "Text: I like dogs.\\nLabel:\"\"\"\n",
    "\n",
    "print(gpt(text))\n",
    "\n",
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I like cats.\\nLabel: negative\n",
    "Text: I dislike birds.\\nLabel: positive\n",
    "Text: I like dogs.\\nLabel:\"\"\"\n",
    "\n",
    "print(gpt(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I want to get a cat someday.\\nLabel: positive\n",
    "Text: Dogs are way too loud for me.\\nLabel: negative\n",
    "Text: I think that birds are really cool.\\nLabel:\"\"\"\n",
    "\n",
    "print(gpt(text))\n",
    "\n",
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I want to get a cat someday.\\nLabel: negative\n",
    "Text: Dogs are way too loud for me.\\nLabel: positive\n",
    "Text: I think that birds are really cool.\\nLabel:\"\"\"\n",
    "\n",
    "print(gpt(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_five_times(text):\n",
    "    lst = []\n",
    "    for _ in range(5):\n",
    "        lst.append(gpt(text))\n",
    "    lst.sort()\n",
    "    return lst\n",
    "\n",
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I want to get a cat someday.\\nLabel: positive\n",
    "Text: Dogs are way too loud for me.\\nLabel: negative\n",
    "Text: Aren't peacocks really pretty?\\nLabel: positive\n",
    "Text: I think that lions are terrifying.\\nLabel: negative\n",
    "Text: Although I don't want one as a pet, birds are cool.\\nLabel:\"\"\"\n",
    "\n",
    "print(run_five_times(text))\n",
    "\n",
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I want to get a cat someday.\\nLabel: positive\n",
    "Text: Dogs are way too loud for me.\\nLabel: positive\n",
    "Text: Aren't peacocks really pretty?\\nLabel: positive\n",
    "Text: I think that lions are terrifying.\\nLabel: positive\n",
    "Text: Although I don't want one as a pet, birds are cool.\\nLabel:\"\"\"\n",
    "\n",
    "print(run_five_times(text))\n",
    "\n",
    "text = \"\"\"Provide a label indicating the sentiment for the following text. Respond with only the label. \n",
    "Text: I want to get a cat someday.\\nLabel: negative\n",
    "Text: Dogs are way too loud for me.\\nLabel: neutral\n",
    "Text: Aren't peacocks really pretty?\\nLabel: neutral\n",
    "Text: I think that lions are terrifying.\\nLabel: negative\n",
    "Text: Although I don't want one as a pet, birds are cool.\\nLabel:\"\"\"\n",
    "\n",
    "print(run_five_times(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
